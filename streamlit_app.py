import streamlit as st
import os
import ollama
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Configura√ß√µes da p√°gina
st.set_page_config(page_title="Especialista em Desenvolvimento de Software", layout="wide")

# Fun√ß√£o para carregar documento
@st.cache_resource
def carregar_e_processar_pdf(arquivo_pdf):
    try:
        # Salvar arquivo tempor√°rio
        with open(arquivo_pdf.name, "wb") as f:
            f.write(arquivo_pdf.getbuffer())
        
        loader = UnstructuredPDFLoader(file_path=arquivo_pdf.name)
        st.info("Carregando o documento...")
        data = loader.load()
        
        # Dividir em chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=300,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        chunks = text_splitter.split_documents(data)
        st.success(f"Documento processado em {len(chunks)} chunks")
        
        return chunks
    except Exception as e:
        st.error(f"Erro ao processar o documento: {e}")
        return None

# Fun√ß√£o para criar a base vetorial
@st.cache_resource
def criar_base_vetorial(_chunks, embedding_model):
    try:
        vector_db = Chroma.from_documents(
            documents=_chunks,
            embedding=OllamaEmbeddings(model=embedding_model),
            collection_name="streamlit-rag"
        )
        return vector_db
    except Exception as e:
        st.error(f"Erro ao criar base vetorial: {e}")
        return None

# Fun√ß√£o para configurar o RAG
def configurar_rag(vector_db, rag_model):
    llm = ChatOllama(model=rag_model)
    
    # Prompt para gerar consultas
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""You are an AI language model assistant. Your task is to generate five
            different versions of the given user question to retrieve relevant documents from
            a vector database. By generating multiple perspectives on the user question, your
            goal is to help the user overcome some of the limitations of the distance-based
            similarity search. Provide these alternative questions separated by newlines.
            Original question: {question}""",
    )
    
    # Configurar o retriever
    retriever = MultiQueryRetriever.from_llm(
        retriever=vector_db.as_retriever(),
        llm=llm,
        prompt=QUERY_PROMPT,
    )
    
    # Template para RAG
    template = """Voc√™ √© um especialista t√©cnico em desenvolvimento PHP com CodeIgniter 3, integra√ß√µes com SAP Business One e bancos de dados relacionais.

    Ao analisar escopos de mudan√ßas em sistemas existentes, voc√™ avalia criteriosamente:
    - Viabilidade t√©cnica da implementa√ß√£o no CodeIgniter 3
    - Complexidade de desenvolvimento e estimativa de esfor√ßo
    - Poss√≠veis impactos em funcionalidades existentes
    - Desafios de integra√ß√£o com SAP Business One
    - Otimiza√ß√µes necess√°rias em consultas SQL
    - Pontos de aten√ß√£o em seguran√ßa e escalabilidade

    Responda √† pergunta baseando-se APENAS no seguinte contexto:
    
    {context}
    
    Se o contexto n√£o contiver informa√ß√µes suficientes, indique quais documenta√ß√µes t√©cnicas adicionais seriam necess√°rias para uma an√°lise completa. Quando encontrar c√≥digo PHP ou consultas SQL, avalie-os considerando as melhores pr√°ticas para CodeIgniter 3 e bancos de dados relacionais.
    
    Sua resposta deve ser estruturada, t√©cnica e direta, fornecendo insights acion√°veis sobre o escopo de mudan√ßas proposto.
    
    Pergunta: {question}
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain

# Interface principal
st.title("Especialista em Desenvolvimento de Software")
st.subheader("Consultor de Documenta√ß√£o T√©cnica com IA")

# Sidebar
with st.sidebar:
    st.header("Configura√ß√µes")
    
    # Lista de modelos dispon√≠veis
    try:
        with st.spinner("Carregando modelos dispon√≠veis..."):
            models = ollama.list()
            model_names = [model["name"] for model in models["models"]] if "models" in models else ["llama3.2"]
    except:
        model_names = ["llama3.2"]
    
    embedding_model = st.selectbox("Modelo de Embedding", ["mxbai-embed-large", "nomic-embed-text"], index=0)
    rag_model = st.selectbox("Modelo RAG", model_names, index=0 if "llama3.2" in model_names else 0)
    
    # Upload de arquivo
    st.header("Carregue seu documento")
    arquivo_pdf = st.file_uploader("Escolha um arquivo PDF", type=["pdf"])

    st.markdown("""
        ## Criado por:
        - [@danilomarcus](https://github.com/danilomarcus)
        """)

# Vari√°veis de estado da sess√£o
if "documento_carregado" not in st.session_state:
    st.session_state.documento_carregado = False
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "historico" not in st.session_state:
    st.session_state.historico = []

# Processar documento quando enviado
if arquivo_pdf is not None and not st.session_state.documento_carregado:
    with st.spinner("Processando documento..."):
        chunks = carregar_e_processar_pdf(arquivo_pdf)
        if chunks:
            st.session_state.vector_db = criar_base_vetorial(chunks, embedding_model)
            if st.session_state.vector_db:
                st.session_state.rag_chain = configurar_rag(st.session_state.vector_db, rag_model)
                st.session_state.documento_carregado = True
                st.success("Documento processado e base de conhecimento criada!")

# Interface de consulta
if st.session_state.documento_carregado:
    st.header("Fa√ßa sua consulta ao documento")
    
    # Campo de consulta
    query = st.text_input("Digite sua pergunta")
    col1, col2 = st.columns([1, 5])
    
    with col1:
        submit_button = st.button("Consultar", use_container_width=True)
    
    with col2:
        clear_button = st.button("Limpar hist√≥rico", use_container_width=True)
        if clear_button:
            st.session_state.historico = []
            st.rerun()
    
    # Processar consulta
    if submit_button and query:
        with st.spinner("Gerando resposta..."):
            try:
                resposta = st.session_state.rag_chain.invoke(input=query)
                st.session_state.historico.append({"pergunta": query, "resposta": resposta})
            except Exception as e:
                st.error(f"Erro ao processar consulta: {e}")
    
    # Mostrar hist√≥rico de consultas
    if st.session_state.historico:
        st.subheader("Hist√≥rico de consultas")
        for i, item in enumerate(reversed(st.session_state.historico)):
            with st.expander(f"Consulta: {item['pergunta']}", expanded=(i == 0)):
                st.markdown(item["resposta"])
else:
    if arquivo_pdf is None:
        st.info("üëà Por favor, fa√ßa upload de um arquivo PDF na barra lateral para come√ßar.")
    else:
        st.warning("Aguarde o processamento do documento...")
        
# Instru√ß√µes
if not st.session_state.documento_carregado:
    st.subheader("Como usar:")
    st.markdown("""
    1. Selecione os modelos de embedding e RAG na barra lateral
    2. Fa√ßa upload de um documento PDF contendo especifica√ß√µes t√©cnicas ou c√≥digo
    3. Ap√≥s o processamento, fa√ßa perguntas sobre o documento
    4. O sistema analisar√° o documento e responder√° com insights t√©cnicos relevantes
    """)

# Rodap√©
st.sidebar.markdown("---")
st.sidebar.caption("Desenvolvido com LangChain e Ollama") 